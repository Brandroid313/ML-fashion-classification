{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Introduction\n",
    " \n",
    " ## Multi class classification a.k.a The clothing condundrum\n",
    " \n",
    " \n",
    "A problem that often comes up in machine leanring is one of classificaiton. Given a set of input, can we group it into a predefined set such as ' puppies ' or ' ice cream ' etc.? \n",
    "## More classes, more problems\n",
    "When given a larger variety of input and output, how well could a machine learning algortihm perform? Especailly with abstract shapes.\n",
    "Here in this notebook I will attempt to train a model that can distugniosh not between just two similar objects, but multiple objects each with variations of their size and shape, namely clothes! \n",
    "## Why clothes?\n",
    "Well for one thing there is a readily availble dataset for them, but also it does present a diffcult challenge for a computer. Think of a t-shirt. Some have patters, while others do not. There is also the types with v-necks and regular necks, but both are still shirts. Would a trained model be able to navigate through the many subtle variations of clothing and correctly figure out which are the most important features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Problem\n",
    "\n",
    "Ever not understand what type of clothing you are looking at? I know I do. Well today we are going to attempt to solve that to blend in as normal humans! What I will attempt to do is to create a model that, looking at the fashion mnsit data set, will be able to classify pictures of clothes into 10 catagories ( 'T-shirt/top', 'Trouser','Pullover','Dress','Coat','Sandal','Shirt', 'Sneaker','Bag', and'Ankle boot' ) and accurately predict what picutre it is likely to be based on that photo.\n",
    "\n",
    "## Input data\n",
    "\n",
    "The input data will be the imaged from the fashion mnist dataset. So that means we will be working with an array of pixels whose value is between 0 and 255. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing our packages and getting our dataset\n",
    "Below is where we get the packages we will be working with, namely keras, the fashion_mnist dataset and models. We will also be getting numpy and matplotlib to help us look a bit closer at some of the data as well as using the sklearn train_test_split to seperate our data into tesstin and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 10:19:39.643404: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-11 10:19:39.643420: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology \n",
    "\n",
    "Below I have outlined the chosen evaluation for the models performance as well as my reasoning behind it. To help clairfy further, there is also a section describing the various functions that were made in order to preapre the data, train the function and measure our success ( or lack there of ) of our training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding our measure of success and evaluation protocol\n",
    "So now that we have our dataset, and our question, we must figure out how we know we have done a good job.\n",
    "## Accuracy, because a shirt should be a shirt\n",
    "I have chosen accuracy to be our model of testing how well we have done ( in the 'metrics' argument of compile ) as what we really want to know is whether or not we have correctly identfied the clothes we are looking through. It is pretty simple as all we need to know is the number of correct predictions versus the number of incorrect predicitons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our helper functions\n",
    "To help keep the code clean and modular, below are functions that have been made to help us along. Here I will explain what they do.\n",
    "\n",
    "## get_accuracy\n",
    "Here we simple get the accuracy of the model and print it out as a percentage.\n",
    "\n",
    "## define_architecture\n",
    "Here is where we build the model. As you can see the model is made of three convolutional layers a two dense layers and we make use of maxpooling and dropout to ( hopefully ) up the accuracey and lower the chances of overfitting.\n",
    "\n",
    "## train\n",
    "Where the magic happens. We take our freshly made model and train it, passing in our metric we have deicded for success, which in this case is accuracey. This function will also print our the accuracy of the model before training we we can compare it with the model performance after.\n",
    "\n",
    "## viz_sample\n",
    "Take a quick look at the data sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model):\n",
    "    # before training accuracy on validation set\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "    percent = round(score * 100, 2)\n",
    "    ratio = round(len(X_test) * score)\n",
    "\n",
    "    print(f'Validation accuracy: {percent}% ({ratio}/{len(X_test)})')\n",
    "\n",
    "\n",
    "def define_architecture():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Three convoultional layes; maxpooling to make a downsampled feature map\n",
    "    model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', \n",
    "                            input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "        \n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.summary()    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train(model, bs=128, epochs=20, valid_split=0.2,\n",
    "          checkpoint='fashion_mnist.hdf5', verbose=True):\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    print('\\n\\nAccuracy before training')\n",
    "    get_accuracy(model)\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath=checkpoint, \n",
    "                                   verbose=1, save_best_only=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, batch_size=bs, epochs=epochs,\n",
    "              validation_split=valid_split, callbacks=[checkpointer],\n",
    "              verbose=verbose, shuffle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_sample(X, y, model=None):\n",
    "    num_gen = lambda x: np.random.randint(0, len(X))\n",
    "    \n",
    "    if not model:\n",
    "        fig, ax = plt.subplots(1, 5, figsize=(10, 7))\n",
    "\n",
    "        for idx in range(5):\n",
    "            N = num_gen(1)\n",
    "            ax[idx].imshow(X[N], cmap='gray')\n",
    "            ax[idx].set_title(labels[y[N]], fontsize=14)\n",
    "            \n",
    "    else:\n",
    "        # show predictions\n",
    "        fig = plt.figure(figsize=(20, 8))\n",
    "        fig.suptitle('True/Predicted values', fontsize=18)\n",
    "        \n",
    "        for i, idx in enumerate(np.random.choice(X_test.shape[0], size=16, replace=False)):\n",
    "            ax = fig.add_subplot(4, 8, i + 1, xticks=[], yticks=[])\n",
    "            ax.imshow(np.squeeze(X_test[idx]), cmap='gray')\n",
    "            pred_idx = np.argmax(y_hat[idx])\n",
    "            true_idx = np.argmax(y_test[idx])\n",
    "            ax.set_title(\"{} ({})\".format(labels[pred_idx], labels[true_idx]),\n",
    "                         color=(\"green\" if pred_idx == true_idx else \"red\"))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "\n",
    "Below are the steps taken to get the data ready and to check it out. First we load up the data and split into testing and training groups, then we must set our labels, which will be our output. Then we check on the data with viz_sample.\n",
    "And at last we get to our preprocessing phase where we rescale the data so that the inputs will be between 0 - 1 and using one hot encoding for our outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and split\n",
    "\n",
    "First, we need to get the data that we are going to work with. Here, I am using Fashion MNIST dataset since it's a little bit more difficult that MNIST, so that it gives enough challenge and is doable at the same time. One alteration I'm going to introduce is the validation split addition; it's important to know how the data does in the training stage. And we will hold out the test set for the very end and not touch it until then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset consists of 60000 instances\n",
      "Test dataset consists of 10000 instances\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "print(f'Train dataset consists of {len(X_train)} instances')\n",
    "print(f'Test dataset consists of {len(X_test)} instances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the labes and Visualize \n",
    "Here I have listed the labels that the model will be using. Below there is a quick look at a sample of the training data to make sure everything is so far working ( not a given when I code! )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the corresponsing labels \n",
    "\n",
    "labels = {\n",
    "    0: 'T-shirt/top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The input values the model expects should be between 0 and 1, how ever since the data we are dealing with comes from images, we need to scale it back. The pixel data comes in values from 0 - 255, so we assign the type to a 32 bit float and divde it by 255 in order to have it in the expected values. Then we assign it to out training and testing variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale images (!)\n",
    "rescale = lambda x: x.astype('float32') / 255.0\n",
    "\n",
    "X_train = rescale(X_train)\n",
    "X_test = rescale(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "As I'm sure you are aware, one hot encoding is where we have an array full of zero's where only the correct choice is given as one. Here we have made sure that the training and testing data have been converted to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before One-Hot encoding:\n",
      "[9 0 0 3 0]\n",
      "\n",
      "After One-Hot encoding:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# use one-hot encoding for categorical outputs\n",
    "onehot = lambda y: np_utils.to_categorical(y, 10)\n",
    "\n",
    "print('Before One-Hot encoding:')\n",
    "print(y_train[:5])\n",
    "\n",
    "y_train = onehot(y_train)\n",
    "y_test = onehot(y_test)\n",
    "\n",
    "print('\\nAfter One-Hot encoding:')\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually training the model\n",
    "Now that we have set up our data, out outputs and our model it is time to train it. Below the 'cnn_model' is going to be our training model ( cnn = Convolutional Neural Network )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "As you can see ( You will have to scroll just a bit ) the accuracy before training is only 9.98% ) So we have our baseline. After running though, how good can we get? I hope it is good, I need a hoody!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 16)        80        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 32)        2080      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 64)          8256      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 3, 3, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3, 3, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               288500    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 303,926\n",
      "Trainable params: 303,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 10:19:55.772203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-11 10:19:55.772881: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-11 10:19:55.773069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-03-11 10:19:55.773221: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-03-11 10:19:55.773376: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-03-11 10:19:55.773528: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-03-11 10:19:55.773671: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-03-11 10:19:55.773812: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-03-11 10:19:55.773954: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-03-11 10:19:55.773977: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-03-11 10:19:55.775138: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy before training\n",
      "Validation accuracy: 9.98% (5988/10000)\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 10:19:56.786006: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 150528000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373/375 [============================>.] - ETA: 0s - loss: 0.6962 - accuracy: 0.7397\n",
      "Epoch 1: val_loss improved from inf to 0.44457, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.6950 - accuracy: 0.7402 - val_loss: 0.4446 - val_accuracy: 0.8413\n",
      "Epoch 2/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.4479 - accuracy: 0.8369\n",
      "Epoch 2: val_loss improved from 0.44457 to 0.36310, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.4478 - accuracy: 0.8369 - val_loss: 0.3631 - val_accuracy: 0.8683\n",
      "Epoch 3/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.3868 - accuracy: 0.8583\n",
      "Epoch 3: val_loss improved from 0.36310 to 0.32216, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.3868 - accuracy: 0.8583 - val_loss: 0.3222 - val_accuracy: 0.8821\n",
      "Epoch 4/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.3510 - accuracy: 0.8709\n",
      "Epoch 4: val_loss improved from 0.32216 to 0.31408, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.3511 - accuracy: 0.8708 - val_loss: 0.3141 - val_accuracy: 0.8819\n",
      "Epoch 5/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.8802\n",
      "Epoch 5: val_loss improved from 0.31408 to 0.31224, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.3247 - accuracy: 0.8801 - val_loss: 0.3122 - val_accuracy: 0.8856\n",
      "Epoch 6/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.3116 - accuracy: 0.8852\n",
      "Epoch 6: val_loss improved from 0.31224 to 0.28654, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.3116 - accuracy: 0.8852 - val_loss: 0.2865 - val_accuracy: 0.8928\n",
      "Epoch 7/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8895\n",
      "Epoch 7: val_loss improved from 0.28654 to 0.27268, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.2982 - accuracy: 0.8895 - val_loss: 0.2727 - val_accuracy: 0.8995\n",
      "Epoch 8/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.8942\n",
      "Epoch 8: val_loss did not improve from 0.27268\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.2868 - accuracy: 0.8942 - val_loss: 0.2744 - val_accuracy: 0.8982\n",
      "Epoch 9/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2781 - accuracy: 0.8988\n",
      "Epoch 9: val_loss improved from 0.27268 to 0.25747, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 0.2781 - accuracy: 0.8988 - val_loss: 0.2575 - val_accuracy: 0.9045\n",
      "Epoch 10/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2690 - accuracy: 0.9010\n",
      "Epoch 10: val_loss did not improve from 0.25747\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2691 - accuracy: 0.9010 - val_loss: 0.2993 - val_accuracy: 0.8903\n",
      "Epoch 11/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.9043\n",
      "Epoch 11: val_loss improved from 0.25747 to 0.25247, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2630 - accuracy: 0.9043 - val_loss: 0.2525 - val_accuracy: 0.9056\n",
      "Epoch 12/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.9054\n",
      "Epoch 12: val_loss improved from 0.25247 to 0.25074, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 0.2577 - accuracy: 0.9054 - val_loss: 0.2507 - val_accuracy: 0.9070\n",
      "Epoch 13/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2513 - accuracy: 0.9065\n",
      "Epoch 13: val_loss did not improve from 0.25074\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2511 - accuracy: 0.9066 - val_loss: 0.2618 - val_accuracy: 0.9073\n",
      "Epoch 14/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2489 - accuracy: 0.9095\n",
      "Epoch 14: val_loss improved from 0.25074 to 0.24205, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 0.2488 - accuracy: 0.9096 - val_loss: 0.2421 - val_accuracy: 0.9112\n",
      "Epoch 15/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2434 - accuracy: 0.9105\n",
      "Epoch 15: val_loss did not improve from 0.24205\n",
      "375/375 [==============================] - 11s 28ms/step - loss: 0.2439 - accuracy: 0.9105 - val_loss: 0.2506 - val_accuracy: 0.9091\n",
      "Epoch 16/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2372 - accuracy: 0.9130\n",
      "Epoch 16: val_loss did not improve from 0.24205\n",
      "375/375 [==============================] - 11s 28ms/step - loss: 0.2372 - accuracy: 0.9130 - val_loss: 0.2437 - val_accuracy: 0.9118\n",
      "Epoch 17/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2344 - accuracy: 0.9154\n",
      "Epoch 17: val_loss improved from 0.24205 to 0.23575, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 11s 28ms/step - loss: 0.2344 - accuracy: 0.9154 - val_loss: 0.2357 - val_accuracy: 0.9165\n",
      "Epoch 18/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2312 - accuracy: 0.9164\n",
      "Epoch 18: val_loss did not improve from 0.23575\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2312 - accuracy: 0.9164 - val_loss: 0.2545 - val_accuracy: 0.9118\n",
      "Epoch 19/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9181\n",
      "Epoch 19: val_loss did not improve from 0.23575\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 0.2303 - accuracy: 0.9181 - val_loss: 0.2443 - val_accuracy: 0.9108\n",
      "Epoch 20/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2272 - accuracy: 0.9178\n",
      "Epoch 20: val_loss did not improve from 0.23575\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2272 - accuracy: 0.9178 - val_loss: 0.2432 - val_accuracy: 0.9115\n",
      "Epoch 21/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2215 - accuracy: 0.9190\n",
      "Epoch 21: val_loss did not improve from 0.23575\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2215 - accuracy: 0.9190 - val_loss: 0.2402 - val_accuracy: 0.9167\n",
      "Epoch 22/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2265 - accuracy: 0.9183\n",
      "Epoch 22: val_loss did not improve from 0.23575\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2265 - accuracy: 0.9183 - val_loss: 0.2397 - val_accuracy: 0.9140\n",
      "Epoch 23/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2209 - accuracy: 0.9214\n",
      "Epoch 23: val_loss did not improve from 0.23575\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2208 - accuracy: 0.9214 - val_loss: 0.2594 - val_accuracy: 0.9054\n",
      "Epoch 24/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2202 - accuracy: 0.9207\n",
      "Epoch 24: val_loss improved from 0.23575 to 0.23482, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2201 - accuracy: 0.9207 - val_loss: 0.2348 - val_accuracy: 0.9147\n",
      "Epoch 25/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.2161 - accuracy: 0.9220\n",
      "Epoch 25: val_loss improved from 0.23482 to 0.23097, saving model to fashion_mnist.hdf5\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2162 - accuracy: 0.9220 - val_loss: 0.2310 - val_accuracy: 0.9145\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "cnn_model = define_architecture()\n",
    "train(cnn_model, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output:\n",
      "Validation accuracy: 91.15% (9115/10000)\n"
     ]
    }
   ],
   "source": [
    "print('Final output:')\n",
    "get_accuracy(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 91.15%\n",
    "\n",
    "Wow, not bad. It isn't the best around, but we started from just a bit under 10% to over 90%! Now I can confidnetly go into my wordrobe and be assured I have a less than 10% chance of wearing my pants on my head!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Well we went from a randoom baseline of just inder 10% to an accuracry rating of over 91%. I'd say this was a farily succesful attempt. But it is still not as high as some models can get, so perhaps some other tweaks could have been made.\n",
    "### Possible changes for improvement\n",
    "One thing that comes to mind is we only ran for 25 epochs, so perhaps given more time we would do better. If possible I would also like to have more training data to access to see if that would make any differnce, but I have limited myself to the mnist training set due to the constraints of my time.\n",
    "\n",
    "### I can find pants\n",
    "Now at least we know that our future robot overloads will be well dressed! What did you think? What would you do differently? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
